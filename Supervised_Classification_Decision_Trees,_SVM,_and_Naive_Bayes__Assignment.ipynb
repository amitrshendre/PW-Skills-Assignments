{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeKcpUvdy1yW"
      },
      "source": [
        "# Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSnsB6opy1yf"
      },
      "source": [
        "Q1. What is Information Gain, and how is it used in Decision Trees?  \n",
        "- **Information Gain (IG)** measures the **reduction in impurity** after a split.\n",
        "- It is calculated as:\n",
        "\n",
        "  \\[\n",
        "  IG = H(Parent) - \\sum_{j} \\frac{N_j}{N} H(Child_j)\n",
        "  \\]\n",
        "\n",
        "- where:  \n",
        "  - \\( H(Parent) \\) is the entropy of the parent node.  \n",
        "  - \\( H(Child_j) \\) is the entropy of the child nodes.  \n",
        "  - \\( N_j \\) is the number of instances in child \\( j \\).  \n",
        "  - \\( N \\) is the total number of instances.\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q2. What is the difference between Gini Impurity and Entropy?  \n",
        "| Measure | Gini Impurity | Entropy |\n",
        "|---------|--------------|---------|\n",
        "| Formula | \\( 1 - \\sum p_i^2 \\) | \\( -\\sum p_i \\log_2 p_i \\) |\n",
        "| Interpretation | Measures misclassification probability | Measures dataset uncertainty |\n",
        "| Computation | Faster | Slower |\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q3. What is Pre-Pruning in Decision Trees?  \n",
        "- **Pre-Pruning** stops the tree from growing too deep.  \n",
        "- It uses **stopping criteria** like:  \n",
        "  - Minimum number of samples per node.  \n",
        "  - Maximum tree depth.  \n",
        "  - Minimum impurity decrease.\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q5. What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm that classifies data by finding the best hyperplane to separate data points into different classes. It is used for both classification and regression tasks, but is especially powerful for classification problems like image and text recognition.\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q6. What is the Kernel Trick in SVM?\n",
        "- The kernel trick is a method used in Support Vector Machines (SVM) to perform non-linear classification by implicitly mapping data into a higher-dimensional space, making it linearly separable. Instead of explicitly computing the new coordinates in this higher dimension, a kernel function computes the dot product between pairs of points in this new space directly from the original data, which is computationally efficient.\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "Q8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- The Naïve Bayes classifier is a simple, probabilistic machine learning algorithm for classification tasks that is based on Bayes' Theorem. It is called \"naïve\" because it makes a strong, often unrealistic, assumption that all features are independent of each other.\n",
        "- A probabilistic classifier: It calculates the probability of a given set of features belonging to a particular class.\n",
        "\n",
        "- Based on Bayes' Theorem: It uses Bayes' theorem to update the probability of a class based on new evidence (the features).\n",
        "\n",
        "- Efficient and fast: This method is computationally efficient, especially for high-dimensional data like text, due to its simplifying assumption.\n",
        "\n",
        "**Why it's called \"naïve\"**\n",
        "\n",
        "- Assumption of independence: The \"naïve\" part of the name comes from its core assumption that the presence of one feature does not affect the presence of any other feature.\n",
        "\n",
        "- Unrealistic in practice: In real-world data, features are often not independent. For example, in a spam email, the words \"free\" and \"winner\" are not independent of each other.\n",
        "\n",
        "- Performance despite the assumption: Even with this strong, unrealistic assumption, the algorithm often performs very well in practice. The independence assumption simplifies the calculations, and even when it's not perfectly true, the resulting class probability estimates are often accurate enough to distinguish between classes.\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "Q9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes?\n",
        "\n",
        " - **Gaussian Naïve Bayes:**\n",
        "\n",
        "- Assumes features follow a normal (Gaussian) distribution\n",
        "\n",
        "- Suitable for continuous data like numerical values\n",
        "\n",
        "- Uses the mean and standard deviation of the feature for each class to calculate the likelihood\n",
        "\n",
        "**Multinomial Naïve Bayes:**\n",
        "\n",
        "- Assumes features are discrete counts\n",
        "\n",
        "- Suitable for data like word frequencies in text documents\n",
        "\n",
        "- Uses the multinomial distribution to model the likelihood of each feature\n",
        "value given a class\n",
        "\n",
        "**Bernoulli Naïve Bayes:**\n",
        "\n",
        "- Assumes features are binary (either present or absent)\n",
        "\n",
        "- Useful for situations where features are classified as \"on\" or \"off\"\n",
        "\n",
        "- Uses the Bernoulli distribution to model the likelihood of a feature being present given a class\n",
        "-----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWazphtXy1yi"
      },
      "source": [
        "# Practical Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Bj58YTy1yn",
        "outputId": "ce027e2e-9daa-49ef-9b46-4602efcbc3f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ],
      "source": [
        "# Practical question\n",
        "'''\n",
        "Q4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "'''\n",
        "'''\n",
        "Answer:-17\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Loading the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Spliting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training Decision Tree with Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "'''\n",
        "Answer:-7\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with a Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with an RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")"
      ],
      "metadata": {
        "id": "czq7FHloYUdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb201ba6-115d-455e-9085-3297c0394203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.9815\n",
            "Accuracy of SVM with RBF Kernel: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "'''\n",
        "Answer:-10\n",
        "'''\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "X = cancer_data.data\n",
        "y = cancer_data.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize and train the Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
        "print(f\"Number of testing samples: {X_test.shape[0]}\")\n",
        "print(f\"Accuracy of the Gaussian Naive Bayes model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnPbSdhHO7C7",
        "outputId": "7728d4a4-2b10-4804-e1ea-00fe529912e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 398\n",
            "Number of testing samples: 171\n",
            "Accuracy of the Gaussian Naive Bayes model: 0.9415\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}